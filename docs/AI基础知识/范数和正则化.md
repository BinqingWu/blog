# 范数与正则化

## 向量的范数-对集合（向量）大小的度量
- 0范数：向量中非零元素的个数
- 1范数：绝对值之和
- 2范数：通常意义上的模
- p范数：
$L_p=\Vert\vec{x}\Vert_p=\sqrt[p]{\sum_{i=1}^{N}|{x_i}|^p}$
- 无穷范数：取向量的极值
    - 正无穷：取极大
    - 负无穷：取极小

## 矩阵的范数-对映射关系变化（矩阵）大小的度量
- 1范数：列绝对值和最大
- 无穷范数：行绝对值和最大
- 2范数：$A^TA$的最大特征值开平方
- 核范数：矩阵奇异值（将矩阵svd分解）的和
    - SVD分解的几何意义就可以做如下的归纳：对于每一个线性映射$\mathcal {T}:K^{n}\rightarrow K^{m}$, $\mathcal {T}$的奇异值分解为在原空间与像空间中分别找到一组标准正交基，使得$\mathcal {T}$把$K^n$的第$i$个基向量映射为$K^m$的第$i$个基向量的非负倍数，并将$K^n$中余下的基向量映射为零向量。
    - ps:特征值是方阵所有，奇异值是所有矩阵。特征值可正可负可为0，奇异值是非负的。特征值对应着到自身空间的变换，及缩放尺度，而奇异值则表示着到另一个空间的变换。
- L0范数：矩阵的非零元素个数
- L1范数：矩阵中每个元素绝对值之和
- L2范数（F范数）：矩阵各元素平方和的平方根
- L21范数：矩阵先以每一列为单位，求每一列的F范数（也可认为是向量的2范数），然后再将得到的结果求L1范数（也可认为是向量的1范数），它是介于L1和L2之间的一种范数
- p范数：$\Vert A\Vert_p=\sqrt[p]{(\sum_{i=1}^m\sum_{j=1}^n{| a_{ij}|}^p)}$


```python
import numpy as np
```


```python
# 2范数
A = np.array([[-1,2,-3],[4,-6,6]])
eig_value, eig_vector = np.linalg.eig(np.dot(A.T,A))
np.sqrt(eig_value.max())
# 10.06227499652064
```


```python
# 核范数
u,sigma,vt = np.linalg.svd(A)
# A是线性变换，v是原空间的正交单位向量组合，u是像空间的正交单位向量组合，sigma是奇异值（v映射后的模长）
sigma.sum()
# 10.928659376802019
```


```python
# numpy中求解范数
_1范数 = np.linalg.norm(x, ord = 1)
_2范数 = np.linalg.norm(x, ord = 2)
_无穷范数 = np.linalg.norm(x, ord = np.inf)
_L2 = np.linalg.norm(x, ord = 'fro')
```

## 范数在正则化中的应用
### 正则化作用

- 防止过拟合。（为什么要防止过拟合，因为我们的目标是训练一个模型使其测试误差小，能准确预测新样本，而不是训练误差小）

- 约束模型特性，将先验知识融入到模型中去（稀疏、低秩、平滑等）。

- 符合奥卡姆剃刀原理：在所有可能选择的模型中，我们应该选择能够很好地解释已知数据并且十分简单的模型

正则化函数 $\Omega(w)$ 有很多种选择，一般是模型复杂度的单调递增函数，模型越复杂，函数值就越大。比如，正则化项可以是模型参数向量的范数。

### 范数与正则化

#### L0范数与L1范数：-> 稀疏（0越多越好）

- L0范数是矩阵中非0元素的个数，最小化L0范数是希望矩阵中有多的零，也就是希望矩阵稀疏。
- L1范数是矩阵中各个元素绝对值之和，也能保证参数的稀疏性
    - [为什么L1能保证稀疏性？](https://zhuanlan.zhihu.com/p/50142573)
        
        - 图像角度：L1正则相当于用菱形去逼近目标，所以更容易引起交点在坐标轴上即得到稀疏解
        
        - 导数角度：假设只有一个参数$w$,损失函数为$L(w)$,加了正则化后为$J_{L 1}(w)=L(w)+\lambda|w|$。假设$L(w)$在0处的导数为$d_0$，引入L1正则项，在0处的导数：
        
          
        $$
        \begin{aligned}
\left.\frac{\partial J_{L 1}(w)}{\partial w}\right|_{w=0^{-}} &=d_{0}-\lambda \\
        \left.\frac{\partial J_{L 1}(w)}{\partial w}\right|_{w=0^{+}} &=d_{0}+\lambda
        \end{aligned}
        $$
        
        
        
        引入L1正则后，代价函数在0处的导数有一个突变，从$d_{0}-\lambda$到$d_{0}+\lambda$。若$d_{0}-\lambda$与$d_{0}+\lambda$异号，则在0处会是一个极小值点。因此，优化时，很可能优化到该极小值点上，即$w$处。
        
        - 概率角度：...
        - **推论**：任何正则化算子，如果他在Wi=0的地方不可微，并且可以分解为一个“求和”的形式，那么这个规则化算子就可以实现稀疏。
    
- 参数稀疏的好处：
    - 重要特征选择：在现实中，输入x大部分特征都与输出y没有关系或者不提供信息。在最小化目标函数时，考虑x这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确y的预测。而参数稀疏化就可以去掉这些无用的信息，只保留最重要的特征。
    - 可解释性更强
    
- L1范数和L0范数可以实现稀疏。但机器学习中，如果为了保证稀疏，会选择L1范数。因为L1范数是L0范数的最优凸近似([???](https://www.zhihu.com/question/40644990))，而且它比L0范数更容易优化求解(L0优化NP难问题)。

#### L2范数:-> 平滑（很多接近0的值）

- L2范数是计算矩阵各元素的平方和的平方根。最小化L2范数，使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0。
    
    - 为什么接近0而不等于0?[???]()
- 优点：让优化求解变得稳定、快速、“唯一”
    - 稳定：更容易求解得到相对well-condition的参数。
        - 什么是well-condition
            - 假设有方程组AX=b，我们需要求解X。如果A或者b稍微的改变，会使得X的解发生很大的改变，那么这个方程组系统就是ill-condition的，反之就是well-condition的。
            - 一般我们的系数矩阵A和b是通过实验数据估计得到，所以它是存在误差的。我们希望我们的系统对这个误差是可以容忍的。但如果系统对这个误差太敏感，会造成我们的解的误差更大，那这个解就太不靠谱了。（表现在训练时候的loss上可能就是剧烈的震荡）
            
        - 为什么可以得到well-condition [???]()
        
            
        
    - 快速：
        
        - [???]()(https://blog.csdn.net/zouxy09/article/details/24971995)
    - 唯一：[???]()

#### L1和L2的比较

- L1: 
    - 趋向于产生少量的特征，其他的特征都是0；
    - 性能鲁棒性强；
    - 解的稳定性弱；
    - 产生多解
- L2: 
    - L2会选择更多的特征，这些特征都会接近于0；
    - 性能鲁棒性弱；
    - 解的稳定性强；
    - 产生唯一解

- 性能鲁棒性是判断的准不准
- 稳定性是判断的稳不稳

- 核函数范数：->(低秩：矩阵的各行或列存在线性依赖，可以投影到更低维的线性子空间)
    - 核范数是rank(w)的凸近似。让这个尽可能的小，可以让求解得到的参数维度更低
    - 常用在矩阵填充（推荐系统）、鲁棒PCA(降噪、背景重建)
    

#### $\lambda$超参

与正则函数在一起的通常还有$\lambda$超参。它主要是平衡loss和正则项这两项的，$\lambda$越大，就表示正则项要比模型训练误差更重要，也就是相比于要模型拟合我们的数据，我们更希望我们的模型能满足我们约束的$\Omega(w)$的特性。反之亦然。

如何定$\lambda$

- 尝试前人经验值
- 大概计算loss项的值是多少，$\Omega(w)$的值是多少，然后针对他们的比例来确定我们的$\lambda$

